// pages/blog/local-llm.js
import { motion } from "framer-motion";
import Link from "next/link";
import Image from "next/image";
import { FaTwitter, FaLinkedin, FaFacebook, FaLink } from "react-icons/fa";
import { NextSeo } from "next-seo";
import FaqSection from "@/components/FaqSection";

const localFaqs = [
  {
    question: "What does it mean to run an LLM locally?",
    answer:
      "Running an LLM locally means executing a large language model directly on your device without sending data to cloud servers.",
  },
  {
    question: "Is local AI better than cloud AI?",
    answer:
      "Local AI offers better privacy, lower long-term costs, and offline access, while cloud AI may be better for very large models or shared infrastructure",
  },
  {
    question: "Can local LLMs run on laptops?",
    answer:
      "We partner with nonprofits, tech companies, sustainability organisations, startups, educational institutions, and mission-driven brands.",
  },
  {
    question: "Is Ollama free to use?",
    answer:
      "Yes. Ollama is an open-source tool that allows users to run and manage LLMs locally.",
  },
];

export default function LocalLLM() {
  return (
    <div className="bg-gray-50 min-h-screen">
      <NextSeo
        title="Run LLMs Locally with Ollama | On-Device AI Tutorial"
        description="Learn how to run LLMs locally with Ollama and other tools. Explore private, offline AI for low-latency performance and local AI model deployment."
        canonical="https://yuvabestudios.com/blog/local-llm"
        openGraph={{
          url: "https://yuvabestudios.com/blog/local-llm",
          title: "Run LLMs Locally with Ollama | On-Device AI Tutorial",
          description:
            "Learn how to run LLMs locally with Ollama and other tools. Explore private, offline AI for low-latency performance and local AI model deployment.",
          images: [
            {
              url: "https://yuvabestudios.com/blog/llm.jpg",
              width: 1200,
              height: 630,
              alt: "Run LLMs Locally with Ollama",
            },
          ],
          site_name: "Yuvabe Studios",
        }}
        twitter={{
          handle: "@YuvabeStudios",
          site: "@YuvabeStudios",
          cardType: "summary_large_image",
        }}
        additionalMetaTags={[
          {
            name: "keywords",
            content:
              "private AI models,local AI inference,edge AI for LLMs,AI without internet,LLMs on personal devices, local AI for businesses, local large language models, offline AI models, local AI, on-device AI, run LLMs locally, Ollama tutorial, private AI, offline AI, low latency AI, local AI models, Hugging Face Transformers local, self-hosted AI models, LLaMA local AI",
          },
        ]}
      />

      {/* Breadcrumbs */}
      <nav className="max-w-6xl mx-auto px-6 py-4 text-sm text-gray-600">
        <ul className="flex space-x-2">
          <li>
            <Link href="/" className="hover:text-[#5829C7]">
              Home
            </Link>
          </li>
          <li>/</li>
          <li>
            <Link href="/blogs" className="hover:text-[#5829C7]">
              Blog
            </Link>
          </li>
          <li>/</li>
          <li className="text-gray-800">Running LLMs Locally</li>
        </ul>
      </nav>

      {/* Meta Info */}
      <div className="max-w-6xl mx-auto px-6 text-black text-sm mb-4">
        <p>
          Published on <span className="text-gray-800">November 11, 2025</span>{" "}
          · 8 min read
        </p>
      </div>

      {/* Title */}
      <div className="max-w-6xl mx-auto px-6 mb-6">
        <motion.h1
          className="text-2xl md:text-5xl font-medium font-primary text-black leading-tight"
          initial={{ opacity: 0, y: 30 }}
          animate={{ opacity: 1, y: 0 }}
          transition={{ duration: 0.8 }}
        >
          The Rise of Local AI: Running LLMs on Your Device
        </motion.h1>

        <p className="text-lg italic text-black font-secondary my-4">
          Running LLMs locally isn’t just about saving costs — it’s about
          privacy, speed, and control. Here’s how this shift is reshaping the AI
          landscape.
        </p>

        {/* Author field */}
        <p className="text-[16px] font-medium text-gray-500 mb-4">
          – By <span className="text-[#5829C7]">Thamarai Kannan</span>, AI/ML
          Developer, Yuvabe Studios
        </p>

        <h3 className="text-xl font-medium font-secondary text-gray-800 mb-0">
          Answer Summary:
        </h3>
        <p className="text-lg  text-black font-secondary mb-4">
          Running LLMs locally allows individuals and businesses to use AI
          directly on their devices without cloud dependency. This approach
          improves privacy, reduces costs, enables offline usage, and delivers
          faster responses compared to cloud-based AI models.
        </p>
      </div>

      {/* Hero Banner Image */}
      <div className="max-w-6xl mx-auto px-6 mb-12">
        <div className="relative w-full h-80  overflow-hidden shadow-lg">
          <Image
            src="/blog/llm.jpg"
            alt="Running LLMs Locally"
            fill
            className="object-cover"
            priority
          />
        </div>
      </div>

      {/* Blog Content */}
      <main className="max-w-6xl mx-auto py-8 px-6 space-y-12">
        <motion.div
          initial={{ opacity: 0, y: 20 }}
          animate={{ opacity: 1, y: 0 }}
          transition={{ duration: 0.6 }}
          className="prose prose-lg prose-indigo"
        >
          {/* Introduction Section */}
          <section className=" rounded-2xl mb-12 font-secondary">
            <h2 className="text-3xl   mb-4 text-[#5829C7] font-medium font-primary">
              Why Running LLMs Locally Is a New Era of AI
            </h2>
            <p className="text-lg leading-relaxed text-black font-secondary">
              When personal computers first arrived in the 1980s, skeptics
              wondered why anyone would need one at home when powerful
              mainframes already existed. History proved them wrong — local
              computing put power directly into the hands of people, sparking a
              digital revolution. We’re witnessing a similar shift today with
              local AI. Until recently, interacting with a Large Language Model
              (LLM) meant sending your queries to the cloud and waiting for a
              response from a massive server farm. Convenient? Yes. But also
              dependent, costly, and not always private. Now imagine carrying
              your own on-device AI assistant inside your laptop — one that
              works even without Wi-Fi, respects your privacy, and answers
              instantly. That’s the promise of running LLMs locally, and it’s
              more than a passing trend. From developers fine-tuning models on
              their MacBooks to small businesses saving lakhs by avoiding API
              fees, local AI is quietly democratizing intelligence. At Yuvabe
              Studios, we’ve been experimenting with this shift, and the results
              are eye-opening. Running models locally isn’t just a technical
              novelty — it feels like the early days of the internet, when
              people realized they could build, create, and control their
              digital experiences on their own terms.
            </p>
          </section>

          {/* Why Run LLMs Locally Section */}
          <section className=" mb-12">
            <h2 className="text-3xl text-[#5829C7] font-medium font-primary mb-6 ">
              Why Local AI Matters
            </h2>

            <h3 className="text-2xl font-medium font-secondary text-gray-800 mb-2">
              1. Privacy by Default
            </h3>
            <p className="mb-4 text-[16px] font-secondary">
              Picture a doctor in a small-town clinic wanting to use AI to draft
              patient reports. With a cloud model, sensitive data must leave the
              clinic and travel to remote servers. With private AI running
              locally, it never leaves the device. That single shift changes the
              trust equation entirely.
            </p>
            <p className="pb-2 text-[16px] font-secondary">
              For industries where data security is critical — healthcare,
              banking, legal services — on-device AI offers peace of mind
              without sacrificing capability.
            </p>

            <h3 className="text-2xl font-medium font-secondary text-gray-800 mb-2">
              2. Cutting Costs Without Losing Capability
            </h3>
            <p className="mb-4 text-[16px] font-secondary">
              One of our startup partners recently shared how their monthly bill
              for AI API calls was higher than their office rent. Switching to a
              local AI model cut that expense to almost zero after the initial
              setup.
            </p>
            <p className="pb-2">
              For businesses that rely heavily on AI — whether for customer
              support, research, or content generation — local AI can feel like
              moving from renting a car by the hour to owning your own vehicle.
              Once the model is downloaded, it can operate indefinitely without
              recurring charges.
            </p>

            <h3 className="text-2xl font-medium font-secondary text-gray-800 mb-2">
              3. Performance that Feels Personal
            </h3>
            <p className="mb-6 text-[16px] font-secondary">
              Anyone who has asked a cloud AI a question during a video call
              knows the awkward pause while the model “thinks.” That lag may be
              just seconds, but it feels longer. On-device LLMs running directly
              on your GPU or Apple Silicon respond almost instantly.
            </p>
            <p className="pb-4 text-[16px] font-secondary">
              For developers working in IDEs, or creators editing videos, that
              speed means flow isn’t interrupted. It makes AI feel less like a
              tool, and more like a collaborator.
            </p>

            <h3 className="text-2xl font-medium font-secondary text-gray-800 mb-2">
              4. AI Without Internet Access.
            </h3>
            <p className="font-secondary text-[16px]">
              A field researcher in Ladakh, a teacher in a rural Tamil Nadu
              classroom, or a journalist reporting from a conflict zone — these
              are not places where you can always rely on stable connectivity.
              Local AI doesn’t care. It works offline, making intelligence
              available anywhere, anytime. By eliminating the need for cloud
              communication, local AI reduces latency, leading to
              near-instantaneous responses. This is crucial for real-time
              applications such as voice assistants, automated coding
              suggestions, and interactive AI-driven tools.
            </p>
          </section>

          {/* Inline Section Image */}
          <section className="mb-12">
            <h2 className="text-3xl font-medium  text-[#5829C7] mb-6 font-primary">
              How to Run LLMs Locally
            </h2>

            <div className="grid grid-cols-1 md:grid-cols-2 gap-8 items-start">
              {/* Left Column - Text Content */}
              <div>
                <h3 className="text-2xl font-medium font-secondary  text-black mb-3">
                  Step 1: Choose the Right Model{" "}
                </h3>
                <p className="mb-2 font-secondary">
                  Several local AI models are optimized for on-device execution,
                  balancing performance and hardware requirements. Popular
                  options include:
                </p>
                <ul className="list-disc pl-6 space-y-2 font-secondary text-black">
                  <li>
                    <strong>LLaMA (Meta AI):</strong> Lightweight yet powerful.
                  </li>
                  <li>
                    <strong>Mistral:</strong> Optimized for inference.
                  </li>
                  <li>
                    <strong>Phi-3 Mini:</strong> Compact and efficient.
                  </li>
                  <li>
                    <strong>GPT-2/3 (smaller variants):</strong> Open-source
                    options for mid-range devices.
                  </li>
                </ul>

                <h3 className="text-2xl font-medium font-secondary text-gray-800 mt-8 mb-3">
                  2. Select the Right Hardware
                </h3>
                <p className="mb-2 font-secondary">
                  Running LLMs locally requires sufficient computational power.
                  The ideal setup depends on the model size and complexity:
                </p>
                <ul className="list-disc pl-6 space-y-2 font-secondary text-black">
                  <li>
                    <strong>CPUs: (smaller variants):</strong> Suitable for
                    small models but may struggle with larger ones.
                  </li>
                  <li>
                    <strong>GPUs: (smaller variants):</strong> The real
                    workhorse for speed. Recommended for faster inference,
                    especially for models with billions of parameters.
                  </li>
                  <li>
                    <strong>
                      Apple Silicon (M1/M2/M3): (smaller variants):
                    </strong>{" "}
                    Surprisingly optimized for local AI workloads, offering high
                    efficiency.
                  </li>
                </ul>

                <h3 className="text-2xl font-medium font-secondary text-gray-800 mt-8 mb-3">
                  Step 3: Interacting With Ollama (CLI, Web, Python)
                </h3>
                <p className="mb-2 font-secondary">
                  To run LLMs efficiently, several optimised inference
                  techniques can be employed:
                </p>
                <ul className="list-disc pl-6 space-y-2 font-secondary text-black">
                  <li>
                    <strong>Quantization: (smaller variants):</strong> Reduces
                    the model’s precision (e.g., from FP32 to INT8), lowering
                    memory usage and improving speed.
                  </li>
                  <li>
                    <strong>
                      Low-Rank Adaptation (LoRA): (smaller variants):
                    </strong>{" "}
                    A fine-tuning method that allows efficient adaptation of
                    large models.
                  </li>
                  <li>
                    <strong>ONNX and TensorRT: (smaller variants):</strong>{" "}
                    Frameworks that optimize inference for different hardware
                    architectures and can dramatically improve speed and reduce
                    resource usage.
                  </li>
                </ul>

                <h3 className="text-2xl font-medium font-secondary text-gray-800 mt-8 mb-3">
                  Step 4: Advanced Customization
                </h3>
                <p className="mb-2 font-secondary">
                  Several frameworks support running LLMs locally:
                </p>
                <ul className="list-disc pl-6 space-y-2 font-secondary text-black">
                  <li>
                    <strong>Ollama: (smaller variants):</strong> A streamlined
                    tool for downloading and running models on personal devices.
                  </li>
                  <li>
                    <strong>LM Studio: (smaller variants):</strong> A
                    user-friendly GUI for local AI experimentation.
                  </li>
                  <li>
                    <strong>
                      Transformers (Hugging Face): (smaller variants):
                    </strong>{" "}
                    Python library for loading and fine-tuning models, including
                    Hugging Face Transformers local deployments.
                  </li>
                  <li>
                    <strong>GGUF and GPTQ: (smaller variants):</strong> Methods
                    for optimizing model execution on various hardware setups.
                  </li>
                </ul>
              </div>

              {/* Right Column - Image */}
              <div className="relative w-full h-full  overflow-hidden ">
                <Image
                  src="/blog/vertical-banner.jpg"
                  alt="Running LLMs Locally Illustration"
                  fill
                  className="object-cover"
                />
              </div>
            </div>
          </section>

          <section className="mb-12">
            <h2 className="text-3xl font-medium font-primary text-[#5829C7] mb-6">
              Spotlight: Running LLMs with Ollama
            </h2>
            <p className="mb-4 font-secondary">
              When we first tried Ollama at Yuvabe Studios, the setup felt
              refreshingly straightforward. Within minutes, we were pulling
              models like LLaMA and running them directly on macOS. For teams or
              individuals wanting to test-drive local AI without drowning in
              configuration, Ollama is one of the easiest on-ramps available.
            </p>
            {/* Hero Image */}
            <div className="relative w-full h-72  overflow-hidden mb-8 ">
              <Image
                src="/blog/ollama.avif"
                alt="Ollama Interface"
                fill
                className="object-cover"
              />
            </div>
            {/* What is Ollama */}
            <h3 className="text-2xl font-medium font-secondary text-gray-800 mb-3">
              What is Ollama?
            </h3>
            <p className="text-black font-secondary mb-6">
              Ollama provides a robust environment for running, modifying, and
              managing LLMs such as LLaMA, Phi, and others optimized for
              different tasks. It supports multiple operating systems—including
              macOS, Linux, and Windows—making it widely accessible. This
              flexibility allows developers to experiment with both open-source
              and custom models without depending on the cloud.
            </p>
            {/* Getting Started */}
            <h3 className="text-xl md:text-3xl font-medium text-[#5829C7] font-primary mb-3">
              Getting Started with Ollama
            </h3>
            <h4 className="text-xl font-medium text-gray-900 mt-6 mb-2">
              Step 1: Download and Install
            </h4>
            <p className="text-black font-secondary mb-4">
              Start by downloading Ollama from the official website.
              Installation is quick and lightweight. Ollama comes with a
              straightforward command-line interface (CLI), allowing you to
              load, configure, and run models directly from your machine.
            </p>
            <h4 className="text-xl font-medium text-gray-900 mt-6 mb-2 font-secondary">
              Step 2: Pull The Model(s)
            </h4>
            <p className="text-black font-secondary mb-4">
              Once installed, pull your desired model from{" "}
              <a
                href="https://ollama.com/library"
                target="_blank"
                rel="noopener noreferrer"
                className="text-blue-600 underline hover:text-blue-800"
              >
                Ollama&apos;s Library
              </a>
              :
            </p>
            <div className="relative">
              <pre className="bg-gray-900 text-green-400 text-sm  p-4 mb-6 overflow-x-auto">
                <code>{`# ollama pull <model> e.g. llama3.2
ollama pull llama3.2`}</code>
              </pre>

              <button
                onClick={() =>
                  navigator.clipboard.writeText(
                    `# ollama pull <model> e.g. llama3.2\nollama pull llama3.2`,
                  )
                }
                className="absolute top-2 right-2 bg-black text-white text-xs px-2 py-1 rounded hover:bg-gray-600"
              >
                Copy
              </button>
            </div>
            <h4 className="text-xl font-medium text-gray-900 mt-6 mb-2 font-secondary">
              Step 3: Run the Model
            </h4>
            <p className="mb-4">
              After pulling, you can run the model immediately:
            </p>
            <div className="relative">
              <pre className="bg-gray-900 text-green-400 text-sm  p-4 mb-6 overflow-x-auto">
                <code>{`#ollama run <model> e.g. llama3.2
ollama run llama3.2
`}</code>
              </pre>

              <button
                onClick={() =>
                  navigator.clipboard.writeText(
                    `# ollama pull <model> e.g. llama3.2\nollama run llama3.2`,
                  )
                }
                className="absolute top-2 right-2 bg-black text-white text-xs px-2 py-1 rounded hover:bg-gray-600"
              >
                Copy
              </button>
            </div>
            <h4 className="text-xl font-medium text-gray-900 mt-6 mb-2 font-secondary">
              Step 4: Interact with the LLM
            </h4>
            <p className="mb-4">
              {" "}
              Ollama gives you multiple ways to interact with your model:
            </p>
            <h5 className="text-lg font-medium text-black mb-2">4.1 CLI</h5>
            <p className="text-black font-secondary mb-4">
              You can now interact with the LLM directly through the
              command-line interface (CLI). Find all the CLI commands at{" "}
              <a
                href="https://github.com/ollama/ollama?tab=readme-ov-file#cli-reference"
                className="text-indigo-600 underline"
                target="_blank"
              >
                Ollama CLI Reference
              </a>
              .
            </p>
            <h5 className="text-lg font-medium text-black mb-2 font-secondary">
              4.2 Web UI
            </h5>
            <p className="text-black font-secondary mb-4">
              Not a fan of chatting with the command line? Ollama also provides
              a REST API! You can seamlessly integrate LLM capabilities into
              your web apps without using the CLI. Just fire up{" "}
              <code className="text-green-600">ollama serve</code> to run Ollama
              without the desktop app.
            </p>
            <div className="relative">
              <pre className="bg-gray-900 text-green-400 text-sm  p-4 mb-6 overflow-x-auto">
                <code>{`curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    { "role": "user", "content": "Why is the sky blue?" }
  ]
}'`}</code>
              </pre>

              <button
                onClick={() =>
                  navigator.clipboard
                    .writeText(`curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    { "role": "user", "content": "Why is the sky blue?" }
  ]
}'`)
                }
                className="absolute top-2 right-2 bg-black text-white text-xs px-2 py-1 rounded hover:bg-gray-600"
              >
                Copy
              </button>
            </div>
            <h5 className="text-lg font-medium text-black mb-2 font-secondary">
              4.3 Python
            </h5>
            <p className="text-black font-secondary mb-4">
              To run Ollama in Python, you can use the{" "}
              <code className="text-green-600">langchain_community</code>{" "}
              library to interact with models like llama3.2. Here’s a quick
              setup example:
            </p>
            <div className="relative">
              <pre className="bg-gray-900 text-green-400 text-sm  p-4 mb-6 overflow-x-auto">
                <code>{`from langchain_community.llms import Ollama

# Initialize Ollama with your chosen model
llm = Ollama(model="llama3.2")

# Invoke the model with a query
response = llm.invoke("What is LLM?")
print(response)`}</code>
              </pre>

              <button
                onClick={() =>
                  navigator.clipboard
                    .writeText(`from langchain_community.llms import Ollama

# Initialize Ollama with your chosen model
llm = Ollama(model="llama3.2")

# Invoke the model with a query
response = llm.invoke("What is LLM?")
print(response)`)
                }
                className="absolute top-2 right-2 bg-black text-white text-xs px-2 py-1 rounded hover:bg-gray-600"
              >
                Copy
              </button>
            </div>
            <h4 className="text-xl font-medium text-black mt-6 mb-2 font-secondary">
              Step 5: Exploring Advanced Features
            </h4>
            <p className="text-black font-secondary mb-4">
              Beyond basic usage, Ollama allows model customization — tuning
              parameters and tailoring execution for specific use cases. This
              makes it valuable for developers building AI tools in specialized
              fields.
            </p>
            <div className="mb-6">
              <h3 className="text-3xl font-medium font-primary text-[#5829C7] mb-4 ">
                Challenges of Running LLMs Locally
              </h3>
              <p className="text-black mb-4">
                Running LLMs locally with Ollama is powerful, but comes with
                some trade-offs:
              </p>
              <ul className="list-disc list-inside space-y-2 text-black font-secondary">
                <li>
                  <strong>Hardware limitations:</strong> Large models may
                  overwhelm average laptops.
                </li>
                <li>
                  <strong>Storage needs:</strong> Multi-gigabyte models can
                  quickly consume disk space.
                </li>
                <li>
                  <strong>Optimization learning curve:</strong> Extracting
                  maximum performance requires tuning.
                </li>
              </ul>
              <p className="text-black mt-4 font-secondary font-medium">
                <strong>The good news?</strong> The open-source ecosystem is
                evolving rapidly. Every few weeks, more efficient, smaller
                models are released, steadily lowering these barriers and making
                local AI increasingly practical.
              </p>
            </div>
            {/* Conclusion */}
            <h3 className="text-3xl font-medium font-primary text-[#5829C7]  mb-3">
              The Future of Local AI
            </h3>
            <p className="text-black font-secondary">
              Local AI isn’t just about where models run—it’s about who holds
              the keys to intelligence. When you run an LLM on your own device,
              you control the data, the costs, and the speed. It’s AI that bends
              to your context, not the other way around.{" "}
            </p>
            <p className="mt-2 text-black font-secondary">
              We’ve seen this story before. The internet moved from mainframes
              to desktops. Music moved from CDs to MP3s. Photography moved from
              film rolls to smartphones. Each shift brought technology closer to
              people.
            </p>{" "}
            <p className="mt-2 text-black font-secondary">
              {" "}
              AI is now following the same arc. And just like those earlier
              revolutions, the winners will be the ones who learn, experiment,
              and adopt early.
            </p>
            <p className="mt-2 text-black font-secondary">
              At Yuvabe Studios, we believe this is the next chapter in making
              technology truly human—intelligence that’s not just powerful, but
              also personal, private, and always within reach.
            </p>
            <div className="mt-8">
              <p className="text-gray-800 font-secondary font-medium mt-8">
                Curious how local AI can be tailored for your business?
                <Link
                  href="/contact"
                  className="text-indigo-600 underline hover:text-indigo-800 transition"
                >
                  {" "}
                  Let’s talk.
                </Link>{" "}
              </p>
            </div>
          </section>

          <FaqSection
            title="Frequently Asked Questions (FAQ)"
            description="Here’s Few things you need to know about LLM"
            faqs={localFaqs}
          />

          {/* Share Section */}
          <div className="border-t pt-6 mt-10">
            <h4 className="text-lg font-semibold text-gray-800 mb-4">
              Share this article:
            </h4>
            <div className="flex flex-wrap gap-4">
              {/* Twitter/X */}
              <a
                href="https://twitter.com/intent/tweet?url=https://yourdomain.com/blog/local-llm&text=Check%20out%20this%20article%20about%20Running%20LLMs%20Locally!"
                target="_blank"
                rel="noopener noreferrer"
                className="flex items-center space-x-2 px-4 py-2 bg-black text-white rounded-lg hover:opacity-80 transition"
              >
                <FaTwitter className="text-white" />
                <span>Twitter</span>
              </a>

              {/* LinkedIn */}
              <a
                href="https://www.linkedin.com/sharing/share-offsite/?url=https://yourdomain.com/blog/local-llm"
                target="_blank"
                rel="noopener noreferrer"
                className="flex items-center space-x-2 px-4 py-2 bg-blue-700 text-white rounded-lg hover:opacity-80 transition"
              >
                <FaLinkedin className="text-white" />
                <span>LinkedIn</span>
              </a>

              {/* Facebook */}
              <a
                href="https://www.facebook.com/sharer/sharer.php?u=https://yourdomain.com/blog/local-llm"
                target="_blank"
                rel="noopener noreferrer"
                className="flex items-center space-x-2 px-4 py-2 bg-blue-600 text-white rounded-lg hover:opacity-80 transition"
              >
                <FaFacebook className="text-white" />
                <span>Facebook</span>
              </a>

              {/* Copy Link */}
              <button
                onClick={() => {
                  navigator.clipboard.writeText(
                    "https://yourdomain.com/blog/local-llm",
                  );
                  alert("Link copied to clipboard!");
                }}
                className="flex items-center space-x-2 px-4 py-2 bg-black text-white rounded-lg hover:opacity-80 transition"
              >
                <FaLink className="text-white" />
                <span>Copy Link</span>
              </button>
            </div>
          </div>
        </motion.div>
      </main>
    </div>
  );
}
